{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "cell_execution_strategy": "setup",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kumarvels/AI-Complete-Learning-Plan/blob/main/DataSpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6Pw4v7REpCw"
      },
      "outputs": [],
      "source": [
        "# prompt: complete clean-up of all existing setup and reinstall start with jdk then spark latest version\n",
        "\n",
        "# Completely remove existing Java installations (use with caution!)\n",
        "!sudo apt-get purge openjdk*\n",
        "!sudo apt-get autoremove\n",
        "\n",
        "# Install the latest JDK (check for the latest version on the official website)\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install openjdk-17-jdk-headless -y\n",
        "\n",
        "# Set JAVA_HOME environment variable (adjust path if necessary)\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"  # Update if needed\n",
        "\n",
        "# Download and install Apache Spark (replace with the latest version)\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.0-bin-hadoop3.tgz\n",
        "\n",
        "# Set environment variables for Spark\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\"\n",
        "os.environ[\"PATH\"] += \":$SPARK_HOME/bin:$SPARK_HOME/sbin\"\n",
        "\n",
        "# Verify the installation\n",
        "!java -version\n",
        "!spark-submit --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://dlcdn.apache.org/spark/spark-3.5.4/spark-3.5.4-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "bTn1Xa5bJ4Ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xf spark-3.5.4-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "yNv5U70NKasy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.4-bin-hadoop3\"\n",
        "os.environ[\"PATH\"] += \":$SPARK_HOME/bin:$SPARK_HOME/sbin\""
      ],
      "metadata": {
        "id": "oUWS4Ql8Knw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install findspark"
      ],
      "metadata": {
        "id": "2tx5oyxcKuPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit --version"
      ],
      "metadata": {
        "id": "vXzRNzyEK0If"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\""
      ],
      "metadata": {
        "id": "Hds_t6h0LWuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.4-bin-hadoop3\""
      ],
      "metadata": {
        "id": "En9B6zljLe7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"PATH\"] += \":$SPARK_HOME/bin:$SPARK_HOME/sbin\""
      ],
      "metadata": {
        "id": "EE-sM36_LjkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo $JAVA_HOME\n",
        "!echo $SPARK_HOME"
      ],
      "metadata": {
        "id": "NgdfSiRULo9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "mSPLhiwfMGk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "NdL4mKJtMLIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MySparkApp\") \\\n",
        "    .config(\"spark.ui.port\", \"4050\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "Mo6wQbIHMQXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Remove existing Java installations\n",
        "!sudo apt-get purge openjdk*\n",
        "!sudo apt-get autoremove\n",
        "!sudo rm -rf /usr/lib/jvm/* # Remove any remaining Java directories\n",
        "\n",
        "# Remove existing Spark installations\n",
        "!sudo rm -rf /content/spark*  # Remove Spark directories in /content\n",
        "!sudo rm -rf /opt/spark*   # Remove Spark directories if installed in /opt (less common)\n",
        "\n",
        "# Remove Spark environment variables (if set)\n",
        "for var in [\"JAVA_HOME\", \"SPARK_HOME\", \"PYSPARK_SUBMIT_ARGS\", \"PYSPARK_PYTHON\"]:\n",
        "  if var in os.environ:\n",
        "    del os.environ[var]\n",
        "\n",
        "# Clear package lists\n",
        "!sudo apt-get update\n",
        "\n",
        "# Install the latest JDK (OpenJDK 17)\n",
        "!sudo apt-get install openjdk-17-jdk-headless -y\n",
        "\n",
        "# Set JAVA_HOME environment variable\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
        "\n",
        "# Download and install the latest Spark (replace with the latest version)\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.0-bin-hadoop3.tgz\n",
        "\n",
        "# Set Spark environment variables\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\"\n",
        "os.environ[\"PATH\"] += \":$SPARK_HOME/bin:$SPARK_HOME/sbin\"\n",
        "\n",
        "# Install pyspark and findspark\n",
        "!pip install pyspark findspark\n",
        "\n",
        "# Initialize findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# Now you can import and use Spark modules\n",
        "from pyspark.sql import SparkSession\n",
        "# ... your Spark code ..."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "jCb293hYLKId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "try:\n",
        "    spark = SparkSession.builder.appName(\"MySparkApp\").getOrCreate()\n",
        "    spark.sparkContext.parallelize(range(10)).count()\n",
        "    print(\"Spark installation and configuration successful!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during Spark test: {e}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "W8vyog7KLUjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "id": "pOB3LMQmMV5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# If the kernel was restarted, you need to redefine the SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MySparkApp\") \\\n",
        "    .config(\"spark.ui.port\", \"4050\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Now you can use the 'spark' object\n",
        "spark"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "9Aa3f0iALh2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Install necessary packages (if not already installed)\n",
        "!pip install pyspark findspark\n",
        "\n",
        "import os\n",
        "# Ensure JAVA_HOME points to the correct Java installation directory\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
        "# Ensure SPARK_HOME points to the correct Spark installation directory\n",
        "# Double-check this path to make sure it's where you extracted Spark\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\"\n",
        "os.environ[\"PATH\"] += \":$SPARK_HOME/bin:$SPARK_HOME/sbin\"\n",
        "\n",
        "import findspark\n",
        "findspark.init(spark_home=os.environ[\"SPARK_HOME\"])  # Explicitly pass SPARK_HOME\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MySparkApp\") \\\n",
        "    .config(\"spark.ui.port\", \"4050\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Test Spark\n",
        "spark.sparkContext.parallelize(range(10)).count()  # Simple Spark operation"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "UKKYy1IiLu7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Install necessary packages (if not already installed)\n",
        "!pip install pyspark findspark\n",
        "\n",
        "import os\n",
        "# Ensure JAVA_HOME points to the correct Java installation directory\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
        "# Ensure SPARK_HOME points to the correct Spark installation directory\n",
        "# Double-check this path to make sure it's where you extracted Spark\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\"\n",
        "os.environ[\"PATH\"] += \":$SPARK_HOME/bin:$SPARK_HOME/sbin\"\n",
        "\n",
        "import findspark\n",
        "findspark.init(spark_home=os.environ[\"SPARK_HOME\"])  # Explicitly pass SPARK_HOME\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MySparkApp\") \\\n",
        "    .config(\"spark.ui.port\", \"4050\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Test Spark\n",
        "spark"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "vTxUSrRoL5ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install pyspark findspark\n",
        "\n",
        "import os\n",
        "# Ensure JAVA_HOME points to the correct Java installation directory\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
        "# Ensure SPARK_HOME points to the correct Spark installation directory\n",
        "# Double-check this path to make sure it's where you extracted Spark\n",
        "# **IMPORTANT:** Remove the extra './' from the path\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\"\n",
        "os.environ[\"PATH\"] += \":$SPARK_HOME/bin:$SPARK_HOME/sbin\"\n",
        "\n",
        "import findspark\n",
        "findspark.init(spark_home=os.environ[\"SPARK_HOME\"])  # Explicitly pass SPARK_HOME\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MySparkApp\") \\\n",
        "    .config(\"spark.ui.port\", \"4050\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Test Spark\n",
        "spark"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "vDCNphO1MCCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
        "!tar xvf spark-3.5.0-bin-hadoop3.tgz\n",
        "!mv spark-3.5.0-bin-hadoop3 /content/\n"
      ],
      "metadata": {
        "id": "HAH8qREXMWvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['SPARK_HOME'] = \"/content/spark-3.5.0-bin-hadoop3\"\n",
        "os.environ['PATH'] += \":/content/spark-3.5.0-bin-hadoop3/bin\"\n"
      ],
      "metadata": {
        "id": "oEJMH33CMaDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark findspark"
      ],
      "metadata": {
        "id": "zgHQDmb0M7nc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "iQMrb2eBM_kL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Spark configuration options\n",
        "spark_conf = {\n",
        "    \"spark.app.name\": \"MySparkApp\",\n",
        "    \"spark.master\": \"local[*]\",  # Use all available cores\n",
        "    \"spark.executor.memory\": \"4g\",  # Adjust memory as needed\n",
        "    \"spark.driver.memory\": \"4g\",   # Adjust memory as needed\n",
        "    \"spark.sql.extensions\": \"io.delta.sql.DeltaSparkSessionExtension\",\n",
        "    \"spark.sql.catalog.spark_catalog\": \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
        "    \"spark.ui.port\": \"4050\",  # Set Spark UI port\n",
        "    # Add more configurations as needed\n",
        "}\n",
        "\n",
        "# Create the SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(spark_conf[\"spark.app.name\"]) \\\n",
        "    .master(spark_conf[\"spark.master\"]) \\\n",
        "    .config(\"spark.executor.memory\", spark_conf[\"spark.executor.memory\"]) \\\n",
        "    .config(\"spark.driver.memory\", spark_conf[\"spark.driver.memory\"]) \\\n",
        "    .config(\"spark.ui.port\", spark_conf[\"spark.ui.port\"]) \\\n",
        "    .config(\"spark.sql.extensions\", spark_conf[\"spark.sql.extensions\"]) \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", spark_conf[\"spark.sql.catalog.spark_catalog\"]) \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "xWfWEa-jNEO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "id": "0Cjo2n40NLTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n"
      ],
      "metadata": {
        "id": "D6z2V-_DNdlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/My Drive/Colab Notebooks/Data/Food_Inspections.csv'  # Replace with your actual path"
      ],
      "metadata": {
        "id": "Bi2GWlwmPRx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O Food_Inspections.csv \"https://data.cityofchicago.org/api/views/4ijn-s7e5/rows.csv?accessType=DOWNLOAD\""
      ],
      "metadata": {
        "id": "PWKGCRv_SI9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "w_eSvT1fTKDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv Food_Inspections.csv \"/content/drive/My Drive/Colab Notebooks/Data/\"  # Replace with your desired path"
      ],
      "metadata": {
        "id": "w2ydWT0sFOGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/drive/My Drive/Colab Notebooks/Data/Food_Inspections.csv\"  # Replace with your actual path"
      ],
      "metadata": {
        "id": "F7jO5ZSCFivq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "5ME6VQtJF2d8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(file_path)  # Assuming 'file_path' is already defined"
      ],
      "metadata": {
        "id": "aLw2ROe0F5eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()  # Displays the first 5 rows of the DataFrame"
      ],
      "metadata": {
        "id": "R8_svD5cGAbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Get the full path including Google Drive details\n",
        "file_path = \"/content/drive/My Drive/Colab Notebooks/Data/Food_Inspections.csv\"  # Update with your actual path\n",
        "\n",
        "# Read the CSV file into a Pandas DataFrame\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first 5 rows of the DataFrame\n",
        "df.head()\n",
        "\n",
        "# Print the full path of the file\n",
        "print(\"Full file path:\", os.path.abspath(file_path))"
      ],
      "metadata": {
        "id": "uSf656cRHyBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Get the full path including Google Drive details\n",
        "file_path = \"/content/drive/My Drive/Colab Notebooks/Data/Food_Inspections.csv\"  # Update with your actual path\n",
        "\n",
        "# Read the first 5 lines of the CSV file\n",
        "df = pd.read_csv(file_path, nrows=5)\n",
        "\n",
        "# Display the first 5 lines\n",
        "print(df)"
      ],
      "metadata": {
        "id": "sLp3P_j9H7FM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gd55RI_OOxLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install openjdk-17-jdk-headless -y  # Or openjdk-17-jdk-headless"
      ],
      "metadata": {
        "id": "HLK2iU62G1RS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\""
      ],
      "metadata": {
        "id": "C0okqdPfG8jF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
        "print(os.environ[\"JAVA_HOME\"])"
      ],
      "metadata": {
        "id": "zq17hyW5JDBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
        "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\n",
        "    \"--master local[*] --conf spark.ui.port=4050 \"\n",
        "    \"--conf spark.driver.extraJavaOptions=-Duser.timezone=UTC \"\n",
        "    \"--conf spark.executor.extraJavaOptions=-Duser.timezone=UTC \"\n",
        "    \"pyspark-shell\"\n",
        ")"
      ],
      "metadata": {
        "id": "A2GU9V3IJM5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Build the SparkSession only once\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Colab\") \\\n",
        "    .config('spark.ui.port', '4050') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Now you can use the 'spark' object for your Spark operations"
      ],
      "metadata": {
        "id": "Qm24fy2bOzo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read a CSV file\n",
        "df = spark.read.csv(\"Food_Inspections.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Show some data\n",
        "df.show(5)"
      ],
      "metadata": {
        "id": "EZGJRQunPQUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Read a CSV file, providing the full path to the file in your Google Drive\n",
        "df = spark.read.csv(\"/content/drive/My Drive/Colab Notebooks/Data/Food_Inspections.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Show some data\n",
        "df.show(5)\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Q7xJOqaFPl3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install pyspark findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.4-bin-hadoop3\" # Update with the Spark version you want to use\n",
        "os.environ[\"PATH\"] += \":$SPARK_HOME/bin:$SPARK_HOME/sbin\"\n",
        "\n",
        "import findspark\n",
        "findspark.init() # Initialize findspark to add Spark to the system path\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MySparkApp\") \\\n",
        "    .config(\"spark.ui.port\", \"4050\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Read a CSV file, providing the full path to the file in your Google Drive\n",
        "df = spark.read.csv(\"/content/drive/My Drive/Colab Notebooks/Data/Food_Inspections.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Show some data\n",
        "df.show(5)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "V4tEM9mhPuc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install pyspark findspark\n",
        "\n",
        "import os\n",
        "# Ensure JAVA_HOME points to a compatible Java version (e.g., Java 17)\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.4-bin-hadoop3\"  # Update with your Spark version\n",
        "os.environ[\"PATH\"] += \":$SPARK_HOME/bin:$SPARK_HOME/sbin\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MySparkApp\") \\\n",
        "    .config(\"spark.ui.port\", \"4050\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Read the CSV file\n",
        "file_path = \"/content/drive/My Drive/Colab Notebooks/Data/Food_Inspections.csv\"\n",
        "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "# Show some data\n",
        "df.show(5)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "J7XTcZmnP8XK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install pyspark findspark\n",
        "\n",
        "import os\n",
        "# Ensure JAVA_HOME points to a compatible Java version (e.g., Java 17)\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
        "# Make sure SPARK_HOME points to the correct Spark installation directory\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.4-bin-hadoop3\"  # Update with your actual Spark installation path\n",
        "os.environ[\"PATH\"] += \":$SPARK_HOME/bin:$SPARK_HOME/sbin\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()  # This will add the Spark libraries to the system path\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MySparkApp\") \\\n",
        "    .config(\"spark.ui.port\", \"4050\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Read the CSV file\n",
        "file_path = \"/content/drive/My Drive/Colab Notebooks/Data/Food_Inspections.csv\"\n",
        "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "# Show some data\n",
        "df.show(5)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "G-pZ1fgcQJ5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install pyspark findspark\n",
        "\n",
        "import os\n",
        "# Ensure JAVA_HOME points to the correct Java installation directory\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
        "# Ensure SPARK_HOME points to the correct Spark installation directory\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.4-bin-hadoop3\"  # Update with your actual Spark installation path\n",
        "os.environ[\"PATH\"] += \":$SPARK_HOME/bin:$SPARK_HOME/sbin\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()  # This initializes findspark and adds Spark to the system path\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MySparkApp\") \\\n",
        "    .config(\"spark.ui.port\", \"4050\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "!pip install pyspark findspark\n",
        "\n",
        "import os\n",
        "# Ensure JAVA_HOME points to the correct Java installation directory\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
        "# Ensure SPARK_HOME points to the correct Spark installation directory\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.4-bin-hadoop3\"  # Update with your actual Spark installation path\n",
        "os.environ[\"PATH\"] += \":$SPARK_HOME/bin:$SPARK_HOME/sbin\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()  # This initializes findspark and adds Spark to the system path\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MySparkApp\") \\\n",
        "    .config(\"spark.ui.port\", \"4050\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Read the CSV file, ensuring the correct file path\n",
        "file_path = \"/content/drive/My Drive/Colab Notebooks/Data/Food_Inspections.csv\"\n",
        "df = spark.read.csv(file_path)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "X9n-alHfQVRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install pyspark findspark\n",
        "\n",
        "import os\n",
        "# Ensure JAVA_HOME points to the correct Java installation directory\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
        "# Ensure SPARK_HOME points to the correct Spark installation directory\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.4-bin-hadoop3\"  # Update with your actual Spark installation path\n",
        "os.environ[\"PATH\"] += \":$SPARK_HOME/bin:$SPARK_HOME/sbin\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()  # This initializes findspark and adds Spark to the system path\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MySparkApp\") \\\n",
        "    .config(\"spark.ui.port\", \"4050\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Read the CSV file, ensuring the correct file path\n",
        "file_path = \"/content/drive/My Drive/Colab Notebooks/Data/Food_Inspections.csv\"\n",
        "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "# Show some data\n",
        "df.show(5)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ysMMIYdvQo4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Remove existing Java installations\n",
        "!sudo apt-get purge openjdk*\n",
        "!sudo apt-get autoremove\n",
        "!sudo rm -rf /usr/lib/jvm/* # Remove any remaining Java directories\n",
        "\n",
        "# Remove existing Spark installations\n",
        "!sudo rm -rf /content/spark*  # Remove Spark directories in /content\n",
        "!sudo rm -rf /opt/spark*   # Remove Spark directories if installed in /opt (less common)\n",
        "\n",
        "# Remove Spark environment variables (if set)\n",
        "for var in [\"JAVA_HOME\", \"SPARK_HOME\", \"PYSPARK_SUBMIT_ARGS\", \"PYSPARK_PYTHON\"]:\n",
        "  if var in os.environ:\n",
        "    del os.environ[var]\n",
        "\n",
        "# Update package lists\n",
        "!sudo apt-get update\n",
        "\n",
        "# Install the latest JDK (OpenJDK 17)\n",
        "!sudo apt-get install openjdk-17-jdk-headless -y\n",
        "\n",
        "# Set JAVA_HOME environment variable\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
        "\n",
        "# Download and install the latest Spark (check for the latest version on the Apache Spark website)\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz  # Replace with the latest version URL\n",
        "!tar xf spark-3.5.0-bin-hadoop3.tgz  # Replace with the latest version file name\n",
        "\n",
        "# Set Spark environment variables\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\"  # Replace with the latest version directory\n",
        "os.environ[\"PATH\"] += \":$SPARK_HOME/bin:$SPARK_HOME/sbin\"\n",
        "\n",
        "# Install findspark\n",
        "!pip install findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# Verify the installation\n",
        "!java -version\n",
        "!spark-submit --version\n",
        "\n",
        "# Restart the kernel (optional but recommended)\n",
        "# In a Jupyter Notebook or Colab"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "32uSvGqeQ7wI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import os\n",
        "\n",
        "# Set JAVA_HOME (if not already set)\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"  # Update if your JDK is in a different location\n",
        "\n",
        "# Set SPARK_HOME (if not already set)\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\"  # Update if your Spark is in a different location\n",
        "\n",
        "# Add Spark's bin and sbin directories to the PATH\n",
        "os.environ[\"PATH\"] += \":$SPARK_HOME/bin:$SPARK_HOME/sbin\"\n",
        "\n",
        "# Verify the PATH\n",
        "print(\"JAVA_HOME:\", os.environ[\"JAVA_HOME\"])\n",
        "print(\"SPARK_HOME:\", os.environ[\"SPARK_HOME\"])\n",
        "print(\"PATH:\", os.environ[\"PATH\"])\n",
        "\n",
        "# Install findspark and initialize it\n",
        "!pip install findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# Now you can import and use Spark modules\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"MySparkApp\").getOrCreate()\n",
        "\n",
        "# Test Spark\n",
        "spark.sparkContext.parallelize(range(10)).count()  # Simple Spark operation"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "AZZkPx87RXt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Install necessary packages (if not already installed)\n",
        "!pip install pyspark findspark\n",
        "\n",
        "import os\n",
        "# Ensure JAVA_HOME points to the correct Java installation directory\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
        "# Ensure SPARK_HOME points to the correct Spark installation directory\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.4-bin-hadoop3\"  # Update with your actual Spark installation path\n",
        "os.environ[\"PATH\"] += \":$SPARK_HOME/bin:$SPARK_HOME/sbin\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()  # This initializes findspark and adds Spark to the system path\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MySparkApp\") \\\n",
        "    .config(\"spark.ui.port\", \"4050\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Test Spark\n",
        "spark.sparkContext.parallelize(range(10)).count()  # Simple Spark operation"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "OFtCaJFzRphW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install pyspark findspark\n",
        "\n",
        "import os\n",
        "# Ensure JAVA_HOME points to the correct Java installation directory\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
        "# Ensure SPARK_HOME points to the correct Spark installation directory\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.4-bin-hadoop3\"  # Update with your actual Spark installation path\n",
        "os.environ[\"PATH\"] += \":$SPARK_HOME/bin:$SPARK_HOME/sbin\"\n",
        "\n",
        "# Before importing findspark, clear any existing Spark context\n",
        "try:\n",
        "    from pyspark import SparkContext\n",
        "    SparkContext.getOrCreate().stop()  # Stop existing Spark context\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "# Remove any Spark related entries from sys.path (findspark's search path)\n",
        "import sys\n",
        "for path in sys.path:\n",
        "    if \"spark\" in path.lower():\n",
        "        sys.path.remove(path)\n",
        "\n",
        "# Now import findspark and initialize it\n",
        "import findspark\n",
        "findspark.init(spark_home=os.environ[\"SPARK_HOME\"])  # Explicitly pass SPARK_HOME\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MySparkApp\") \\\n",
        "    .config(\"spark.ui.port\", \"4050\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Test Spark\n",
        "spark.sparkContext.parallelize(range(10)).count()  # Simple Spark operation"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "69ZUlu2ySDTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ItY8iCxcOzTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove existing Java installations\n",
        "!sudo apt-get purge openjdk*\n",
        "!sudo apt-get autoremove\n",
        "!sudo rm -rf /usr/lib/jvm/* # Remove any remaining Java directories\n",
        "\n",
        "# Remove existing Spark installations\n",
        "!sudo rm -rf /content/spark*  # Remove Spark directories in /content\n",
        "!sudo rm -rf /opt/spark*   # Remove Spark directories if installed in /opt (less common)\n",
        "\n",
        "# Remove Spark environment variables (if set)\n",
        "for var in [\"JAVA_HOME\", \"SPARK_HOME\", \"PYSPARK_SUBMIT_ARGS\", \"PYSPARK_PYTHON\"]:\n",
        "  if var in os.environ:\n",
        "    del os.environ[var]\n",
        "\n",
        "# Clear package lists\n",
        "!sudo apt-get update"
      ],
      "metadata": {
        "id": "mgumJ680ejH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the latest JDK (OpenJDK 17)\n",
        "!sudo apt-get install openjdk-17-jdk-headless -y\n",
        "\n",
        "# Set JAVA_HOME environment variable\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
        "\n",
        "# Download and install the latest Spark (replace with the latest version)\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.0-bin-hadoop3.tgz\n",
        "\n",
        "# Set Spark environment variables\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\"\n",
        "os.environ[\"PATH\"] += \":$SPARK_HOME/bin:$SPARK_HOME/sbin\"\n",
        "\n",
        "!pip install pyspark findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# Now you can import and use Spark modules\n",
        "from pyspark.sql import SparkSession\n",
        "# ... your Spark code ..."
      ],
      "metadata": {
        "id": "z0Qizam3es6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install pyspark\n",
        "!pip install findspark\n",
        "!pip install pyngrok"
      ],
      "metadata": {
        "id": "fid7t0xJe-aL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!dpkg -l"
      ],
      "metadata": {
        "id": "uddJK2djfZfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!dpkg -l | grep -i python  # List packages containing \"python\"\n",
        "!dpkg -l | grep -i java    # List packages containing \"java\"\n",
        "!dpkg -l | grep -i spark   # List packages containing \"spark\""
      ],
      "metadata": {
        "id": "EkXNtERcfd0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!dpkg -l | grep -i spark   # List packages containing \"spark\""
      ],
      "metadata": {
        "id": "zx4rMcg4fm_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the latest JDK (OpenJDK 17)\n",
        "!sudo apt-get install openjdk-17-jdk-headless -y\n",
        "\n",
        "# Set JAVA_HOME environment variable\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
        "\n",
        "# Download and install the latest Spark (replace with the latest version)\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.0-bin-hadoop3.tgz\n",
        "\n",
        "# Set Spark environment variables\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\"\n",
        "os.environ[\"PATH\"] += \":$SPARK_HOME/bin:$SPARK_HOME/sbin\"\n",
        "\n",
        "!pip install pyspark findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# Now you can import and use Spark modules\n",
        "from pyspark.sql import SparkSession\n",
        "# ... your Spark code ..."
      ],
      "metadata": {
        "id": "yoWm4ImuftvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Java Development Kit (JDK)\n",
        "!apt-get install openjdk-17-jdk-headless -qq > /dev/null  # Install OpenJDK 17, suppress output\n",
        "\n",
        "# Set JAVA_HOME environment variable\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"  # Path to OpenJDK 17\n",
        "\n",
        "# Download Spark (choose the desired version and Hadoop compatibility)\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz # Spark 3.5.0 with Hadoop 3\n",
        "\n",
        "# Extract Spark archive\n",
        "!tar -xzf spark-3.5.0-bin-hadoop3.tgz\n",
        "\n",
        "# Set SPARK_HOME environment variable\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\" # Path to extracted Spark directory\n",
        "\n",
        "# Add Spark bin directory to PATH\n",
        "os.environ[\"PATH\"] += \":$SPARK_HOME/bin\"\n",
        "\n",
        "# Install findspark\n",
        "!pip install -q findspark\n",
        "\n",
        "# Initialize findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# Verify installation\n",
        "!java -version\n",
        "!spark-submit --version"
      ],
      "metadata": {
        "id": "5aomStiZgdPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(f\"Java Home: {os.getenv('JAVA_HOME')}\")\n",
        "print(f\"Spark Home: {os.getenv('SPARK_HOME')}\")\n"
      ],
      "metadata": {
        "id": "M-SRiqMzhxKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        " !pip install -q pyspark\n",
        " !pip install -q findspark\n"
      ],
      "metadata": {
        "id": "LvHkqvQvhzvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "0ky0_ibUh9Cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Environment Variables\n",
        "num_cores = os.getenv('NUM_CORES', '2')\n",
        "spark_port = os.getenv('SPARK_UI_PORT', '4050')\n",
        "\n",
        "# Spark Configuration\n",
        "try:\n",
        "    spark = SparkSession.builder \\\n",
        "        .master(f\"local[{num_cores}]\") \\\n",
        "        .appName(\"Colab\") \\\n",
        "        .config(\"spark.ui.port\", spark_port) \\\n",
        "        .getOrCreate()\n",
        "\n",
        "    # Additional Configurations\n",
        "    spark.conf.set(\"spark.executor.memory\", \"2g\")\n",
        "    spark.conf.set(\"spark.executor.cores\", num_cores)\n",
        "    spark.conf.set(\"spark.driver.memory\", \"2g\")\n",
        "\n",
        "    # Logger Setup\n",
        "    import logging\n",
        "    logger = logging.getLogger('pyspark')\n",
        "    logger.setLevel(logging.INFO)\n",
        "    logger.info(\"Spark session started with the following configuration:\")\n",
        "    logger.info(f\"Number of Cores: {num_cores}\")\n",
        "    logger.info(f\"Spark UI Port: {spark_port}\")\n",
        "\n",
        "    # Verification\n",
        "    print(\"Spark Session Configurations:\")\n",
        "    print(f\"Number of Cores: {num_cores}\")\n",
        "    print(f\"Spark UI Port: {spark_port}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Error:\", e)\n"
      ],
      "metadata": {
        "id": "GTWVPXO1iBiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.2/hadoop-client-api-3.3.2.jar -P /path/to/your/spark/jars/\n"
      ],
      "metadata": {
        "id": "AhuJEN-_ihzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"SPARK_HOME\"] = \"local[*]\"\n",
        "os.environ[\"PYTHONPATH\"] = f\"{os.environ['SPARK_HOME']}/python:{os.environ['SPARK_HOME']}/python/lib/py4j-0.10.9.5.src.zip\"\n"
      ],
      "metadata": {
        "id": "VRi8uI9XiqTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install findspark\n",
        "import findspark\n",
        "findspark.init()\n"
      ],
      "metadata": {
        "id": "t1QpZTvEi6KJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Colab\") \\\n",
        "    .config(\"spark.ui.port\", \"4050\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"Spark Session Created successfully\")\n"
      ],
      "metadata": {
        "id": "QG3WhrOvi-Uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall pyspark -y\n",
        "!pip install pyspark==3.3.2\n"
      ],
      "metadata": {
        "id": "zh3LnMYXjIxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(f\"SPARK_HOME: {os.getenv('SPARK_HOME')}\")\n"
      ],
      "metadata": {
        "id": "rH7CHwNhjMnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(f\"SPARK_HOME: {os.getenv('SPARK_HOME')}\")\n"
      ],
      "metadata": {
        "id": "kj4edZwtj-GK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -L -o /content/spark-3.3.2-bin-hadoop3/jars/hadoop-client-api-3.3.2.jar \"https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.2/hadoop-client-api-3.3.2.jar\"\n"
      ],
      "metadata": {
        "id": "_qWDp_LQkA_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!java -version\n",
        "!pip show py4j\n"
      ],
      "metadata": {
        "id": "1c_0z9_kkFtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Simplified Configuration\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Colab\") \\\n",
        "    .config(\"spark.ui.port\", \"4050\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"Spark Session Created successfully\")\n"
      ],
      "metadata": {
        "id": "32C2-TsUkK9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "TqSurcgTnwyg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}